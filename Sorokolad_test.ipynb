{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Esg1dwjZqcdt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 585.6617 | Val Loss: 55.0509| Train 0.6633139533942086: -45973.4637| Val 0.6633139533942086: 0.6772\n",
      "Epoch 2/50: Train Loss: 64.6762 | Val Loss: 160.5799| Train 0.5817145893301111: 0.7712| Val 0.5817145893301111: 0.5828\n",
      "Epoch 3/50: Train Loss: 55.6874 | Val Loss: 16.3107| Train 0.9474273683055049: 0.8201| Val 0.9474273683055049: 0.9429\n",
      "Epoch 4/50: Train Loss: 48.8659 | Val Loss: 12.8478| Train 0.954338269489594: 0.8422| Val 0.954338269489594: 0.9567\n",
      "Epoch 5/50: Train Loss: 49.5163 | Val Loss: 15.3332| Train 0.9330990493751598: 0.8391| Val 0.9330990493751598: 0.9394\n",
      "Epoch 6/50: Train Loss: 46.0027 | Val Loss: 19.9775| Train 0.8925795011457462: 0.8523| Val 0.8925795011457462: 0.9038\n",
      "Epoch 7/50: Train Loss: 47.7236 | Val Loss: 18.0919| Train 0.9180675987673155: 0.8473| Val 0.9180675987673155: 0.9228\n",
      "Epoch 8/50: Train Loss: 46.5596 | Val Loss: 11.1253| Train 0.9459977317836645: 0.8506| Val 0.9459977317836645: 0.9494\n",
      "Epoch 9/50: Train Loss: 43.7195 | Val Loss: 27.4123| Train 0.9197763480721308: 0.8614| Val 0.9197763480721308: 0.9167\n",
      "Epoch 10/50: Train Loss: 44.2190 | Val Loss: 7.1989| Train 0.9660879605328415: 0.8571| Val 0.9660879605328415: 0.9741\n",
      "Epoch 11/50: Train Loss: 43.3118 | Val Loss: 14.9720| Train 0.9334455754153624: 0.8601| Val 0.9334455754153624: 0.9501\n",
      "Epoch 12/50: Train Loss: 42.0932 | Val Loss: 35.5198| Train 0.763191699691674: 0.8644| Val 0.763191699691674: 0.7931\n",
      "Epoch 13/50: Train Loss: 43.3445 | Val Loss: 43.3773| Train 0.8286725019984509: 0.8608| Val 0.8286725019984509: 0.8671\n",
      "Epoch 14/50: Train Loss: 42.3089 | Val Loss: 12.2877| Train 0.9686571634476717: 0.8638| Val 0.9686571634476717: 0.9606\n",
      "Epoch 15/50: Train Loss: 40.1853 | Val Loss: 30.5368| Train 0.9296633929892074: 0.8726| Val 0.9296633929892074: 0.9209\n",
      "Epoch 16/50: Train Loss: 41.7190 | Val Loss: 27.4305| Train 0.871438210678266: 0.8662| Val 0.871438210678266: 0.8897\n",
      "Epoch 17/50: Train Loss: 41.7546 | Val Loss: 19.7447| Train 0.9194317278632793: 0.8663| Val 0.9194317278632793: 0.9303\n",
      "Epoch 18/50: Train Loss: 41.4290 | Val Loss: 15.6079| Train 0.9457343879873216: 0.8665| Val 0.9457343879873216: 0.9457\n",
      "Epoch 19/50: Train Loss: 39.6039 | Val Loss: 18.9516| Train 0.9431707843437137: 0.8723| Val 0.9431707843437137: 0.9329\n",
      "Epoch 20/50: Train Loss: 41.7809 | Val Loss: 24.3662| Train 0.9093472153311977: 0.8651| Val 0.9093472153311977: 0.8942\n",
      "Epoch 21/50: Train Loss: 39.7838 | Val Loss: 11.2127| Train 0.9664665117521194: 0.8725| Val 0.9664665117521194: 0.9635\n",
      "Epoch 22/50: Train Loss: 42.0781 | Val Loss: 14.7191| Train 0.9442596706228462: 0.8642| Val 0.9442596706228462: 0.9521\n",
      "Epoch 23/50: Train Loss: 40.0655 | Val Loss: 20.9973| Train 0.9109263761102336: 0.8725| Val 0.9109263761102336: 0.9437\n",
      "Epoch 24/50: Train Loss: 39.7841 | Val Loss: 9.2290| Train 0.9670349538437136: 0.8725| Val 0.9670349538437136: 0.9626\n",
      "Epoch 25/50: Train Loss: 39.7512 | Val Loss: 49.2418| Train 0.8623557360964148: 0.8732| Val 0.8623557360964148: 0.8757\n",
      "Epoch 26/50: Train Loss: 40.0410 | Val Loss: 13.1554| Train 0.9621139935595258: 0.8712| Val 0.9621139935595258: 0.9612\n",
      "Epoch 27/50: Train Loss: 40.2941 | Val Loss: 98.5708| Train 0.6913973737436011: 0.8719| Val 0.6913973737436011: 0.6885\n",
      "Epoch 28/50: Train Loss: 38.2547 | Val Loss: 16.5482| Train 0.9446382950588935: 0.8768| Val 0.9446382950588935: 0.9349\n",
      "Epoch 29/50: Train Loss: 39.5218 | Val Loss: 25.3949| Train 0.9304198532709068: 0.8739| Val 0.9304198532709068: 0.9365\n",
      "Epoch 30/50: Train Loss: 39.2381 | Val Loss: 27.9707| Train 0.8572902853457006: 0.8734| Val 0.8572902853457006: 0.8512\n",
      "Epoch 31/50: Train Loss: 40.2049 | Val Loss: 42.9546| Train 0.8195538266507768: 0.8699| Val 0.8195538266507768: 0.8755\n",
      "Epoch 32/50: Train Loss: 38.0124 | Val Loss: 36.0559| Train 0.8865556436660339: 0.8788| Val 0.8865556436660339: 0.8899\n",
      "Epoch 33/50: Train Loss: 40.4479 | Val Loss: 28.8048| Train 0.9154940380693661: 0.8709| Val 0.9154940380693661: 0.9081\n",
      "Epoch 34/50: Train Loss: 41.0873 | Val Loss: 19.2940| Train 0.9270395854149406: 0.8687| Val 0.9270395854149406: 0.9378\n",
      "Epoch 35/50: Train Loss: 40.0260 | Val Loss: 20.8477| Train 0.9126503219743323: 0.8722| Val 0.9126503219743323: 0.9318\n",
      "Epoch 36/50: Train Loss: 38.8880 | Val Loss: 16.1826| Train 0.9464349769527697: 0.8736| Val 0.9464349769527697: 0.9415\n",
      "Epoch 37/50: Train Loss: 40.3326 | Val Loss: 51.0670| Train 0.8519394179013614: 0.8716| Val 0.8519394179013614: 0.8593\n",
      "Epoch 38/50: Train Loss: 38.7177 | Val Loss: 10.7140| Train 0.9639837263699046: 0.8759| Val 0.9639837263699046: 0.9596\n",
      "Epoch 39/50: Train Loss: 38.3859 | Val Loss: 14.2400| Train 0.9540917920664014: 0.8759| Val 0.9540917920664014: 0.9368\n",
      "Epoch 40/50: Train Loss: 39.8530 | Val Loss: 31.6890| Train 0.9363679867261997: 0.8744| Val 0.9363679867261997: 0.9142\n",
      "Epoch 41/50: Train Loss: 40.8901 | Val Loss: 42.2764| Train 0.8126365831226936: 0.8666| Val 0.8126365831226936: 0.7974\n",
      "Epoch 42/50: Train Loss: 40.0845 | Val Loss: 9.6409| Train 0.9778755953987521: 0.8720| Val 0.9778755953987521: 0.9694\n",
      "Epoch 43/50: Train Loss: 38.7028 | Val Loss: 16.5147| Train 0.9317857196947218: 0.8752| Val 0.9317857196947218: 0.9448\n",
      "Epoch 44/50: Train Loss: 39.2890 | Val Loss: 43.7545| Train 0.779515763932085: 0.8722| Val 0.779515763932085: 0.8684\n",
      "Epoch 45/50: Train Loss: 40.7238 | Val Loss: 29.1323| Train 0.9138680226698508: 0.8690| Val 0.9138680226698508: 0.9016\n",
      "Epoch 46/50: Train Loss: 40.2670 | Val Loss: 7.1091| Train 0.9687091669415503: 0.8701| Val 0.9687091669415503: 0.9741\n",
      "Epoch 47/50: Train Loss: 38.7410 | Val Loss: 10.9728| Train 0.9490702507691152: 0.8757| Val 0.9490702507691152: 0.9543\n",
      "Epoch 48/50: Train Loss: 39.8077 | Val Loss: 15.2073| Train 0.9609116523904301: 0.8718| Val 0.9609116523904301: 0.9583\n",
      "Epoch 49/50: Train Loss: 39.9654 | Val Loss: 26.0222| Train 0.921549102563063: 0.8697| Val 0.921549102563063: 0.9241\n",
      "Epoch 50/50: Train Loss: 40.1006 | Val Loss: 10.5619| Train 0.9531937101906416: 0.8722| Val 0.9531937101906416: 0.9529\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Sorokolad.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fJIu9zDXqcdw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XvLelUBpqcdy",
    "outputId": "9c9743c6-1106-4a9a-9f77-fdef49e1ffcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_AvgPool2d (__main__.TestLayers.test_AvgPool2d) ... ok\n",
      "test_BatchNormalization (__main__.TestLayers.test_BatchNormalization) ... ok\n",
      "test_ClassNLLCriterion (__main__.TestLayers.test_ClassNLLCriterion) ... ok\n",
      "test_ClassNLLCriterionUnstable (__main__.TestLayers.test_ClassNLLCriterionUnstable) ... ok\n",
      "test_Conv2d (__main__.TestLayers.test_Conv2d) ... ok\n",
      "test_Dropout (__main__.TestLayers.test_Dropout) ... ok\n",
      "test_ELU (__main__.TestLayers.test_ELU) ... ok\n",
      "test_Flatten (__main__.TestLayers.test_Flatten) ... ok\n",
      "test_Gelu (__main__.TestLayers.test_Gelu) ... ok\n",
      "test_LeakyReLU (__main__.TestLayers.test_LeakyReLU) ... ok\n",
      "test_Linear (__main__.TestLayers.test_Linear) ... ok\n",
      "test_LogSoftMax (__main__.TestLayers.test_LogSoftMax) ... ok\n",
      "test_MaxPool2d (__main__.TestLayers.test_MaxPool2d) ... ok\n",
      "test_Sequential (__main__.TestLayers.test_Sequential) ... ok\n",
      "test_SoftMax (__main__.TestLayers.test_SoftMax) ... ok\n",
      "test_SoftPlus (__main__.TestLayers.test_SoftPlus) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 16 tests in 2040.093s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=16 errors=0 failures=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLayers(unittest.TestCase):\n",
    "    def test_Linear(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
    "            custom_layer = Linear(n_in, n_out)\n",
    "            custom_layer.W = torch_layer.weight.data.numpy()\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softmax(dim=1)\n",
    "            custom_layer = SoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "            next_layer_grad = next_layer_grad.clip(1e-5,1.)\n",
    "            next_layer_grad = 1. / next_layer_grad\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "\n",
    "    def test_LogSoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "            custom_layer = LogSoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_BatchNormalization(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 32, 16\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            alpha = 0.9\n",
    "            custom_layer = BatchNormalization(alpha)\n",
    "            custom_layer.train()\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n",
    "            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "\n",
    "            # 3. check moving mean\n",
    "            self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))\n",
    "            # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "            #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "            # 4. check evaluation mode\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.evaluate()\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            torch_layer.eval()\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "    def test_Sequential(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 0.9\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n",
    "            torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n",
    "            custom_layer = Sequential()\n",
    "            bn_layer = BatchNormalization(alpha)\n",
    "            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.add(bn_layer)\n",
    "            scaling_layer = ChannelwiseScaling(n_in)\n",
    "            scaling_layer.gamma = torch_layer.weight.data.numpy()\n",
    "            scaling_layer.beta = torch_layer.bias.data.numpy()\n",
    "            custom_layer.add(scaling_layer)\n",
    "            custom_layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5)) #понизила точность с 1e-6 до 1e-5\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_Dropout(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            p = np.random.uniform(0.3, 0.7)\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0),\n",
    "                                        np.isclose(layer_output*(1.-p), layer_input))))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0),\n",
    "                                        np.isclose(layer_grad*(1.-p), next_layer_grad))))\n",
    "\n",
    "            # 3. check evaluation mode\n",
    "            layer.evaluate()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            # 4. check mask\n",
    "            p = 0.0\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            p = 0.5\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n",
    "\n",
    "            # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "            batch_size, n_in = 1000, 1\n",
    "            p = 0.8\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "            layer_input = layer_input.T\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "    def test_Conv2d(self):\n",
    "        hyperparams = [\n",
    "            {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n",
    "             'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'},\n",
    "            {'batch_size': 4, 'in_channels': 1, 'out_channels': 2, 'height': 28, 'width': 28,\n",
    "             'kernel_size': 5, 'stride': 2, 'padding': 2, 'bias': False, 'padding_mode': 'replicate'},\n",
    "            #несовместимые параметры stride и padding для оригинальной функции, так что меняю stride с 2 до 1\n",
    "            {'batch_size': 16, 'in_channels': 3, 'out_channels': 3, 'height': 64, 'width': 64,\n",
    "             'kernel_size': 3, 'stride': 1, 'padding': 'same', 'bias': True, 'padding_mode': 'reflect'},\n",
    "            {'batch_size': 2, 'in_channels': 3, 'out_channels': 8, 'height': 10, 'width': 10,\n",
    "             'kernel_size': 2, 'stride': (1,2), 'padding': 0, 'bias': True, 'padding_mode': 'zeros'},\n",
    "        ]\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for _ in range(100):\n",
    "          for params in hyperparams:\n",
    "              with self.subTest(params=params):\n",
    "\n",
    "                  batch_size = params['batch_size']\n",
    "                  in_channels = params['in_channels']\n",
    "                  out_channels = params['out_channels']\n",
    "                  height = params['height']\n",
    "                  width = params['width']\n",
    "                  kernel_size = params['kernel_size']\n",
    "                  stride = params['stride']\n",
    "                  padding = params['padding']\n",
    "                  bias = params['bias']\n",
    "                  padding_mode = params['padding_mode']\n",
    "\n",
    "                  custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                        stride=stride, padding=padding, bias=bias,\n",
    "                                        padding_mode=padding_mode)\n",
    "                  custom_layer.train()\n",
    "\n",
    "                  torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                                stride=stride, padding=padding, bias=bias,\n",
    "                                                padding_mode=padding_mode)\n",
    "\n",
    "                  custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n",
    "                  if bias:\n",
    "                      custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n",
    "\n",
    "                  layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n",
    "                  input_var = torch.tensor(layer_input, requires_grad=True)\n",
    "\n",
    "                  custom_output = custom_layer.updateOutput(layer_input)\n",
    "                  torch_output = torch_layer(input_var)\n",
    "                  self.assertTrue(\n",
    "                      np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "                  next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "                  custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "                  torch_output.backward(torch.tensor(next_layer_grad))\n",
    "                  torch_grad = input_var.grad.detach().numpy()\n",
    "                  self.assertTrue(\n",
    "                      np.allclose(torch_grad, custom_grad, atol=1e-5))\n",
    "\n",
    "\n",
    "    def test_LeakyReLU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            torch_layer = torch.nn.LeakyReLU(slope)\n",
    "            custom_layer = LeakyReLU(slope)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ELU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 1.0\n",
    "            torch_layer = torch.nn.ELU(alpha)\n",
    "            custom_layer = ELU(alpha)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftPlus(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softplus()\n",
    "            custom_layer = SoftPlus()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ClassNLLCriterionUnstable(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "            layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input /= layer_input.sum(axis=-1, keepdims=True)\n",
    "            layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(torch.log(layer_input_var),\n",
    "                                                 #неподходящий формат входных данных для оригинальной функции (добавляем \".long()\")\n",
    "                                                 Variable(torch.from_numpy(target_labels).long(), requires_grad=False)) \n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ClassNLLCriterion(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterion()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var,\n",
    "                                                 #неподходящий формат входных данных для оригинальной функции (добавляем \".long()\")\n",
    "                                                 Variable(torch.from_numpy(target_labels).long(), requires_grad=False))\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "\n",
    "    def test_MaxPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, channels, height, width = 4, 3, 16, 16\n",
    "        kernel_size, stride, padding = 2, 2, 0\n",
    "\n",
    "        for _ in range(100):\n",
    "          custom_module = MaxPool2d(kernel_size, stride, padding)\n",
    "          custom_module.train()\n",
    "\n",
    "          torch_module = torch.nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          custom_output = custom_module.updateOutput(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_grad, custom_grad, atol=1e-5))\n",
    "\n",
    "    def test_AvgPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, channels, height, width = 4, 3, 16, 16\n",
    "        kernel_size, stride, padding = 3, 2, 1\n",
    "\n",
    "        for _ in range(100):\n",
    "          custom_module = AvgPool2d(kernel_size, stride, padding)\n",
    "          custom_module.train()\n",
    "\n",
    "          torch_module = torch.nn.AvgPool2d(kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          custom_output = custom_module.updateOutput(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_grad, custom_grad, atol=1e-5))\n",
    "\n",
    "    def test_Flatten(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        test_params = [\n",
    "            {'start_dim': 1, 'end_dim': -1},\n",
    "            {'start_dim': 2, 'end_dim': 3},\n",
    "            {'start_dim': 0, 'end_dim': -1},\n",
    "        ]\n",
    "\n",
    "        for _ in range(100):\n",
    "          for params in test_params:\n",
    "              with self.subTest(params=params):\n",
    "                  start_dim = params['start_dim']\n",
    "                  end_dim = params['end_dim']\n",
    "\n",
    "                  custom_module = Flatten(start_dim, end_dim)\n",
    "                  input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)\n",
    "                  input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "                  custom_output = custom_module.updateOutput(input_np)\n",
    "                  torch_output = torch.flatten(input_var, start_dim=start_dim, end_dim=end_dim)\n",
    "                  self.assertTrue(\n",
    "                      np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "                  next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "                  custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "                  torch_output.backward(torch.tensor(next_grad))\n",
    "                  torch_grad = input_var.grad.detach().numpy()\n",
    "                  self.assertTrue(\n",
    "                      np.allclose(torch_grad, custom_grad, atol=1e-6))\n",
    "\n",
    "    def test_Gelu(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for _ in range(100):\n",
    "          custom_module = Gelu()\n",
    "          custom_module.train()\n",
    "\n",
    "          torch_module = torch.nn.GELU()\n",
    "\n",
    "          input_np = np.random.randn(10, 5).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          custom_output = custom_module.updateOutput(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-2)) #поменяла точность с 1e-6 до 1e-2\n",
    "\n",
    "          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_grad, custom_grad, atol=1e-2)) #поменяла точность с 1e-5 до 1e-2\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncR3rbxJqcd-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
