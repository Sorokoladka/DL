{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "3MeKai5Xj6eX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwlFrG-Tj6eY"
   },
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "W8BLmtZ3j6eZ"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box)\n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`:\n",
    "\n",
    "        output = module.forward(input)\n",
    "\n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule.\n",
    "\n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "\n",
    "        This includes\n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "\n",
    "        Make sure to both store the data in `output` field and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        # The easiest case:\n",
    "\n",
    "        # self.output = input\n",
    "        # return self.output\n",
    "\n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input.\n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "\n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "\n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        # The easiest case:\n",
    "\n",
    "        # self.gradInput = gradOutput\n",
    "        # return self.gradInput\n",
    "\n",
    "        pass\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKRkIjT8j6eZ"
   },
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb98PPpJj6ea"
   },
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially.\n",
    "\n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "\n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "\n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})\n",
    "\n",
    "\n",
    "        Just write a little loop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = input\n",
    "        for module in self.modules:\n",
    "            self.output = module.forward(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "\n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)\n",
    "            gradInput = module[0].backward(input, g_1)\n",
    "\n",
    "\n",
    "        !!!\n",
    "\n",
    "        To ech module you need to provide the input, module saw while forward pass,\n",
    "        it is used while computing gradients.\n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
    "        and NOT `input` to this Sequential module.\n",
    "\n",
    "        !!!\n",
    "\n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################\n",
    "        outputs = [input]\n",
    "        for i in range(len(self.modules)):\n",
    "            outputs.append(self.modules[i].forward(outputs[-1]))\n",
    "        current_grad = gradOutput\n",
    "        for i in range(len(self.modules)-1, -1, -1):\n",
    "            current_grad = self.modules[i].backward(outputs[i], current_grad)\n",
    "        self.gradInput = current_grad\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "\n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "\n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfXdYfO4j6ea"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuwvBkuNj6ea"
   },
   "source": [
    "## 1 (0.2). Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "D0uoyqkpj6ea"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation\n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
    "\n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = input @ self.W.T + self.b\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput @ self.W\n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradW += gradOutput.T @ input\n",
    "        self.gradb += gradOutput.sum(axis=0)\n",
    "        pass\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNOnHXZJj6eb"
   },
   "source": [
    "## 2. (0.2) SoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
    "\n",
    "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "VIValI0hj6eb"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        norm_input = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.exp(norm_input) / np.exp(norm_input).sum(axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.updateOutput(input)\n",
    "        batch_size = input.shape[0]\n",
    "        gradInput = np.zeros_like(input)\n",
    "        for i in range(batch_size):\n",
    "            softmax_output = self.output[i].reshape(-1, 1)\n",
    "            grad_output = gradOutput[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(softmax_output) - softmax_output @ softmax_output.T\n",
    "            gradInput[i] = (jacobian @ grad_output).flatten()\n",
    "        self.gradInput = gradInput\n",
    "        return self.gradInput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy3DJjynj6eb"
   },
   "source": [
    "## 3. (0.2) LogSoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
    "\n",
    "The main goal of this layer is to be used in computation of log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Xo7DRdAJj6eb"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        norm_input = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = norm_input - np.log(np.exp(norm_input).sum(axis=1, keepdims=True))\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.updateOutput(input)\n",
    "        self.gradInput = gradOutput - np.exp(self.output) * gradOutput.sum(axis=1, keepdims=True)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP5QdmmPj6eb"
   },
   "source": [
    "## 4. (0.3) Batch normalization\n",
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
    "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
    "```\n",
    "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
    "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
    "```\n",
    "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
    "\n",
    "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "fGTTDqVgj6eb"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = None\n",
    "        self.moving_variance = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        # use self.EPS please\n",
    "        if self.training:\n",
    "            batch_mean = np.mean(input, axis=0)\n",
    "            batch_variance = np.var(input, axis=0, ddof=0)\n",
    "            self.output = (input - batch_mean) / np.sqrt(batch_variance + self.EPS)\n",
    "            if self.moving_mean is None:\n",
    "                self.moving_mean = batch_mean\n",
    "                self.moving_variance = batch_variance\n",
    "            else:\n",
    "                self.moving_mean = self.alpha * self.moving_mean + (1 - self.alpha) * batch_mean\n",
    "                self.moving_variance = self.alpha * self.moving_variance + (1 - self.alpha) * batch_variance\n",
    "        else:\n",
    "            self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            batch_mean = np.mean(input, axis=0)\n",
    "            batch_variance = np.var(input, axis=0)\n",
    "            sigma = np.sqrt(batch_variance + self.EPS)\n",
    "            m = input.shape[0]  # batch size\n",
    "            mean_grad = np.mean(gradOutput, axis=0)\n",
    "            covar_grad = np.mean(gradOutput * (input - batch_mean), axis=0) / (sigma**2)\n",
    "            self.gradInput = (gradOutput - mean_grad - (input - batch_mean) * covar_grad) / sigma\n",
    "            self.gradGamma = np.sum((input - batch_mean) * gradOutput / sigma, axis=0)\n",
    "            self.gradBeta = np.sum(gradOutput, axis=0)\n",
    "        else:\n",
    "            self.gradInput = gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "8XUS3Lt-j6eb"
   },
   "outputs": [],
   "source": [
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "       Implements linear transform of input y = \\gamma * x + \\beta\n",
    "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "\n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * self.gamma + self.beta\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * self.gamma\n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
    "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA5zjM3jj6eb"
   },
   "source": [
    "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gackeo1cj6eb"
   },
   "source": [
    "## 5. (0.3) Dropout\n",
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "NmLQV3jXj6eb"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*input.shape) > self.p).astype(float)\n",
    "            self.output = input * self.mask / (1 - self.p)\n",
    "        else:\n",
    "            self.output = input\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            self.gradInput = gradOutput * self.mask / (1 - self.p)\n",
    "        else:\n",
    "            self.gradInput = gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WHGIqJFlhz2"
   },
   "source": [
    "# 6. (2.0) Conv2d\n",
    "Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "c1RjNoEXlOHP"
   },
   "outputs": [],
   "source": [
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, bias=True, padding_mode='zeros'):\n",
    "        super(Conv2d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
    "        if padding == 'same':\n",
    "          self.padding = (int((self.kernel_size[0] - 1) / 2), int((self.kernel_size[1] - 1) / 2))\n",
    "        else:\n",
    "          self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
    "        self.bias_TF = bias\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "        self.weight = np.random.randn(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
    "        self.gradW = np.zeros_like(self.weight)\n",
    "        if self.bias_TF:\n",
    "          self.bias = np.random.randn(out_channels)\n",
    "          self.gradb = np.zeros_like(self.bias)\n",
    "\n",
    "        if self.padding_mode=='zeros':\n",
    "          self.mode='constant'\n",
    "          self.constant_values=0\n",
    "        elif self.padding_mode=='replicate':\n",
    "          self.mode='edge'\n",
    "        elif self.padding_mode=='reflect':\n",
    "          self.mode='reflect'\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size = input.shape[0]\n",
    "        H_in, W_in = input.shape[2], input.shape[3]\n",
    "        H_out = int((H_in + 2 * self.padding[0] - self.kernel_size[0]) / self.stride[0]) + 1\n",
    "        W_out = int((W_in + 2 * self.padding[1] - self.kernel_size[1]) / self.stride[1]) + 1\n",
    "        self.output = np.zeros((batch_size, self.out_channels, H_out, W_out))\n",
    "\n",
    "        pad_config = ((0, 0), (0, 0), (self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]))\n",
    "        if self.mode == 'constant':\n",
    "            self.input_pad = np.pad(input, pad_config, mode=self.mode, constant_values=self.constant_values)\n",
    "        else:\n",
    "            self.input_pad = np.pad(input, pad_config, mode=self.mode)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            for c in range(self.out_channels):\n",
    "                for h in range(H_out):\n",
    "                    h_start = h * self.stride[0]\n",
    "                    for w in range(W_out):\n",
    "                        w_start = w * self.stride[1]\n",
    "                        field = self.input_pad[n, :, h_start:h_start+self.kernel_size[0], \n",
    "                                             w_start:w_start+self.kernel_size[1]]\n",
    "                        self.output[n, c, h, w] = np.sum(field * self.weight[c])\n",
    "                if self.bias_TF:\n",
    "                    self.output[n, c] += self.bias[c]\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size = input.shape[0]\n",
    "        H_in, W_in = input.shape[2], input.shape[3]\n",
    "        _, _, H_out, W_out = gradOutput.shape\n",
    "        self.gradInput = np.zeros_like(input)\n",
    "        for n in range(batch_size):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for h_out in range(H_out):\n",
    "                    h_start = h_out * self.stride[0]\n",
    "                    for w_out in range(W_out):\n",
    "                        w_start = w_out * self.stride[1]\n",
    "                        for c_in in range(self.in_channels):\n",
    "                            for h_k in range(self.kernel_size[0]):\n",
    "                                h_in = h_start + h_k - self.padding[0]\n",
    "                                for w_k in range(self.kernel_size[1]):\n",
    "                                    w_in = w_start + w_k - self.padding[1]\n",
    "                                    if 0 <= h_in < H_in and 0 <= w_in < W_in:\n",
    "                                        self.gradInput[n, c_in, h_in, w_in] += (\n",
    "                                            gradOutput[n, c_out, h_out, w_out] * \n",
    "                                            self.weight[c_out, c_in, h_k, w_k]\n",
    "                                        )\n",
    "                                    else:\n",
    "                                        if self.mode == 'edge':\n",
    "                                            h_clip = max(0, min(h_in, H_in - 1))\n",
    "                                            w_clip = max(0, min(w_in, W_in - 1))\n",
    "                                            self.gradInput[n, c_in, h_clip, w_clip] += (\n",
    "                                                gradOutput[n, c_out, h_out, w_out] * \n",
    "                                                self.weight[c_out, c_in, h_k, w_k]\n",
    "                                            )\n",
    "                                        elif self.mode == 'reflect':\n",
    "                                            h_ref = h_in\n",
    "                                            if h_in < 0: h_ref = -h_in\n",
    "                                            elif h_in >= H_in: h_ref = 2 * H_in - h_in - 2\n",
    "                                            w_ref = w_in\n",
    "                                            if w_in < 0: w_ref = -w_in\n",
    "                                            elif w_in >= W_in: w_ref = 2 * W_in - w_in - 2\n",
    "                                            self.gradInput[n, c_in, h_ref, w_ref] += (\n",
    "                                                gradOutput[n, c_out, h_out, w_out] * \n",
    "                                                self.weight[c_out, c_in, h_k, w_k]\n",
    "                                            )\n",
    "        \n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        H_in, W_in = input.shape[-2:]\n",
    "        batch_size, C_out, H_out, W_out = gradOutput.shape\n",
    "\n",
    "        self.gradInput = np.zeros_like(input)\n",
    "\n",
    "        for n in range(batch_size):\n",
    "          for c_out in range(self.out_channels):\n",
    "            for h_out in range(H_out):\n",
    "              for w_out in range(W_out):\n",
    "                h_start = h_out * self.stride[0]\n",
    "                w_start = w_out * self.stride[1]\n",
    "\n",
    "                receptive_field = self.input_pad[n, :, h_start:(h_start + self.kernel_size[0]), w_start: (w_start+ self.kernel_size[1])]\n",
    "\n",
    "                self.gradW[c_out] += receptive_field * gradOutput[n, c_out, h_out, w_out]\n",
    "                if self.bias_TF:\n",
    "                  self.gradb[c_out] += gradOutput[n, c_out, h_out, w_out]\n",
    "        pass\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        if self.bias_TF:\n",
    "          self.gradb.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        if self.bias_TF:\n",
    "          return [self.weight, self.bias]\n",
    "        else: return [self.weight]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        if self.bias_TF:\n",
    "          return [self.gradW, self.gradb]\n",
    "        else: return [self.gradW]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Conv2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "updUVZE9qixP"
   },
   "source": [
    "# 7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html).\n",
    "Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Qys58EzkqhLj"
   },
   "outputs": [],
   "source": [
    "class MaxPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
    "        stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
    "        padding = self.padding if isinstance(self.padding, tuple) else (self.padding, self.padding)\n",
    "        out_height = (in_height + 2 * padding[0] - kernel_size[0]) // stride[0] + 1\n",
    "        out_width = (in_width + 2 * padding[1] - kernel_size[1]) // stride[1] + 1\n",
    "        self.output = np.zeros((batch_size, in_channels, out_height, out_width))\n",
    "        self.indices = np.zeros((batch_size, in_channels, out_height, out_width, 2), dtype=int)\n",
    "        input_padded = np.pad(input, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), mode='constant', constant_values=-np.inf)\n",
    "    \n",
    "        for b in range(batch_size):\n",
    "            for c in range(in_channels):\n",
    "                for oh in range(out_height):\n",
    "                    for ow in range(out_width):\n",
    "                        h_start = oh * stride[0]\n",
    "                        h_end = h_start + kernel_size[0]\n",
    "                        w_start = ow * stride[1]\n",
    "                        w_end = w_start + kernel_size[1]\n",
    "                        receptive_field = input_padded[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, oh, ow] = np.max(receptive_field)\n",
    "                        max_index = np.unravel_index(np.argmax(receptive_field), receptive_field.shape)\n",
    "                        self.indices[b, c, oh, ow] = [h_start + max_index[0], w_start + max_index[1]]\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
    "        stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
    "        padding = self.padding if isinstance(self.padding, tuple) else (self.padding, self.padding)\n",
    "        self.gradInput = np.zeros_like(input)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(in_channels):\n",
    "                for oh in range(gradOutput.shape[2]):\n",
    "                    for ow in range(gradOutput.shape[3]):\n",
    "                        h, w = self.indices[b, c, oh, ow]\n",
    "                        self.gradInput[b, c, h, w] += gradOutput[b, c, oh, ow]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MaxPool2d\"\n",
    "\n",
    "\n",
    "class AvgPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(AvgPool2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
    "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        batch_size, channels, in_h, in_w = input.shape\n",
    "        out_h = (in_h + 2*self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1\n",
    "        out_w = (in_w + 2*self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1\n",
    "        if any(self.padding):\n",
    "            pad_h, pad_w = self.padding\n",
    "            padded_input = np.pad(input, \n",
    "                               ((0,0), (0,0),\n",
    "                               (pad_h, pad_h),\n",
    "                               (pad_w, pad_w)))\n",
    "        else:\n",
    "            padded_input = input\n",
    "        self.output = np.empty((batch_size, channels, out_h, out_w))\n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(out_h):\n",
    "                    h_start = h * self.stride[0]\n",
    "                    for w in range(out_w):\n",
    "                        w_start = w * self.stride[1]\n",
    "                        window = padded_input[n, c, \n",
    "                                            h_start:h_start+self.kernel_size[0],\n",
    "                                            w_start:w_start+self.kernel_size[1]]\n",
    "                        self.output[n, c, h, w] = window.mean()\n",
    "                        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        batch_size, channels, out_h, out_w = gradOutput.shape\n",
    "        in_h, in_w = input.shape[2:]\n",
    "        kernel_area = self.kernel_size[0] * self.kernel_size[1]\n",
    "        self.gradInput = np.zeros_like(input)\n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h_out in range(out_h):\n",
    "                    h_in = h_out * self.stride[0] - self.padding[0]\n",
    "                    for w_out in range(out_w):\n",
    "                        w_in = w_out * self.stride[1] - self.padding[1]\n",
    "                        grad = gradOutput[n, c, h_out, w_out] / kernel_area\n",
    "                        for h_k in range(self.kernel_size[0]):\n",
    "                            for w_k in range(self.kernel_size[1]):\n",
    "                                hi = h_in + h_k\n",
    "                                wi = w_in + w_k\n",
    "                                if 0 <= hi < in_h and 0 <= wi < in_w:\n",
    "                                    self.gradInput[n, c, hi, wi] += grad\n",
    "                                    \n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"AvgPool2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTN5R3CwrukV"
   },
   "source": [
    "# 8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**.\n",
    "They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool2d(Module):\n",
    "    def __init__(self, keepdims):\n",
    "        super(GlobalMaxPool2d, self).__init__()\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size, C, H, W = input.shape\n",
    "        self.output = np.max(input, axis=(2, 3))\n",
    "        input_1 = input.reshape(batch_size, C, -1)\n",
    "        self.max_indices = np.argmax(input_1, axis=2)\n",
    "        if self.keepdims:\n",
    "          self.output = self.output.reshape((batch_size, C, 1, 1))\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size, C = gradOutput.shape[:2]\n",
    "        H, W = input.shape[-2:]\n",
    "        self.gradInput = np.zeros_like(input)\n",
    "        if self.keepdims:\n",
    "            gradOutput = gradOutput.reshape(batch_size, C)\n",
    "        max_h = self.max_indices // W\n",
    "        max_w = self.max_indices % W\n",
    "        batch, channel = np.indices((batch_size, C))\n",
    "        self.gradInput[batch, channel, max_h, max_w] = gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"GlobalMaxPool2d\"\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(Module):\n",
    "    def __init__(self, keepdims):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size, C, H, W = input.shape\n",
    "        self.output = np.average(input, axis=(2, 3))\n",
    "        if self.keepdims:\n",
    "          self.output = self.output.reshape((batch_size, C, 1, 1))\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size, C = gradOutput.shape[:2]\n",
    "        H, W = input.shape[-2:]\n",
    "        n_pix = H*W\n",
    "        self.gradInput = np.zeros_like(input)\n",
    "        if self.keepdims:\n",
    "            gradOutput = gradOutput.reshape(batch_size, C)\n",
    "        for n in range(batch_size):\n",
    "            for c in range(C):\n",
    "              self.gradInput[n, c] = gradOutput[n,c] / n_pix\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"GlobalAvgPool2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYeBQDBhtViy"
   },
   "source": [
    "# 9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "SimPEMOFqhTQ"
   },
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    def __init__(self, start_dim=0, end_dim=-1):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        input_shape = input.shape\n",
    "        start_dim = self.start_dim\n",
    "        end_dim = self.end_dim if self.end_dim >= 0 else len(input_shape) + self.end_dim\n",
    "        preserved_dims = input_shape[:start_dim]\n",
    "        flattened_dims = input_shape[start_dim:end_dim+1]\n",
    "        remaining_dims = input_shape[end_dim+1:] if end_dim+1 < len(input_shape) else ()\n",
    "        flattened_size = 1\n",
    "        for dim in flattened_dims:\n",
    "            flattened_size *= dim\n",
    "        output_shape = preserved_dims + (flattened_size,) + remaining_dims\n",
    "        self.in_shape = input_shape\n",
    "        self.output = input.reshape(output_shape)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput.reshape(self.in_shape)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Flatten\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o36vPHSSj6eb"
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_pryRQIj6ec"
   },
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "sgm8bXjKj6ec"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB0UHGagj6ec"
   },
   "source": [
    "## 10. (0.1) Leaky ReLU\n",
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "agwfkwO0j6ec"
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "\n",
    "        self.slope = slope\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.where(input > 0, input, self.slope * input)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.where(input > 0, gradOutput, self.slope * gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-STyecvj6ec"
   },
   "source": [
    "## 11. (0.1) ELU\n",
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "jJSzEu1mj6ec"
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.where(input > 0, input, self.alpha * (np.exp(input) - 1))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.where(input > 0, gradOutput, self.alpha * np.exp(input) * gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn3C7KTqj6ec"
   },
   "source": [
    "## 12. (0.1) SoftPlus\n",
    "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "xcDPMssrj6ec"
   },
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.log1p(np.exp(input))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = (1.0 / (1.0 + np.exp(-input))) * gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw3PeZjOuo0e"
   },
   "source": [
    "# 13. (0.2) Gelu\n",
    "Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "SdieE0Dtuo8j"
   },
   "outputs": [],
   "source": [
    "class Gelu(Module):\n",
    "    def __init__(self):\n",
    "        super(Gelu, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = 0.5 * input * (1 + np.tanh((2 / np.pi)**0.5 * (input + 0.044715 * input**3)))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ###############################################\n",
    "        x = input\n",
    "        sqrt_2_pi = np.sqrt(2 / np.pi)\n",
    "        alpha = 0.044715\n",
    "        \n",
    "        x_cubed = x**3\n",
    "        r = sqrt_2_pi * (x + alpha * x_cubed)\n",
    "        tanh_r = np.tanh(r)\n",
    "        dtanh_dr = 1 - tanh_r**2\n",
    "        dr_dx = sqrt_2_pi * (1 + 3 * alpha * x**2)\n",
    "        dtanh_dx = dtanh_dr * dr_dx\n",
    "        gelu_grad = 0.5 * (1 + tanh_r) + 0.5 * x * dtanh_dx\n",
    "        self.gradInput = gradOutput * gelu_grad\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Gelu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55p7UvPAj6ec"
   },
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NFaxZaqj6ec"
   },
   "source": [
    "Criterions are used to score the models answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "XGu45A8qj6ec"
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function\n",
    "            associated to the criterion and return the result.\n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result.\n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuU26xkpj6ec"
   },
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- target: **`batch_size x n_feats`**\n",
    "- output: **scalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "-i3VNuHhj6ec"
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8LKLWNVj6ec"
   },
   "source": [
    "## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n",
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
    "- input:   **`batch_size x n_feats`** - probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        log_probs = -np.log(input_clamp)\n",
    "        batch_losses = np.sum(log_probs * target, axis=1)\n",
    "        total_loss = np.sum(batch_losses)\n",
    "        batch_size = input.shape[0]\n",
    "        self.output = total_loss / batch_size\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        input_clamp = np.clip(input, self.EPS, 1.0 - self.EPS)\n",
    "        batch_size = input.shape[0]\n",
    "        grad = -target / input_clamp\n",
    "        self.gradInput = grad / batch_size\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHr_JbU5j6ec"
   },
   "source": [
    "## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n",
    "- input:   **`batch_size x n_feats`** - log probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n",
    "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "v7N8bVP9j6ec"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_scores = np.sum(input * target, axis=1)\n",
    "        batch_size = input.shape[0]\n",
    "        total_loss = -np.sum(batch_scores)\n",
    "        self.output = total_loss / batch_size\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        batch_size = input.shape[0]\n",
    "        self.gradInput = -target / batch_size\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-ZnhKxaj6ed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC2Bf1PP2Ios"
   },
   "source": [
    "1-  :  ,     - 5 . \\\\\n",
    "2-  :     .   :\n",
    "  1.     ,      . - 1 .\n",
    "  2.        .  FCNN, dropout, batchnorm, MSE.    .     ,    . - 1 .\n",
    "  3.      MNIST.  , , ,  - 1 .\n",
    "  4.     .       , ,   . - 2 . \\\\\n",
    "\n",
    "      :\n",
    "1.       .\n",
    "2.       -.     .\n",
    "3.    lr.\n",
    "4.  .\n",
    "5.        .\n",
    "6.   ()  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_upd(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sequential_upd, self).__init__()\n",
    "        self.modules = []\n",
    "\n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        current_output = input\n",
    "        if not self.modules:\n",
    "            self.output = current_output\n",
    "            return self.output\n",
    "        for module in self.modules:\n",
    "            current_output = module.forward(current_output)\n",
    "        self.output = current_output\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        current_grad = gradOutput\n",
    "        if not self.modules:\n",
    "            self.gradInput = current_grad\n",
    "            return self.gradInput\n",
    "        for i in range(len(self.modules) - 1, -1, -1):\n",
    "            module = self.modules[i]\n",
    "            input_for_backward = input if i == 0 else self.modules[i - 1].output\n",
    "            current_grad = module.backward(input_for_backward, current_grad)\n",
    "        self.gradInput = current_grad\n",
    "        return self.gradInput\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "\n",
    "    def getParameters(self):\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            module_params = module.getParameters()\n",
    "            if module_params:\n",
    "                params.extend(module_params)\n",
    "        return params\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        grad_params = []\n",
    "        for module in self.modules:\n",
    "            module_grad_params = module.getGradParameters()\n",
    "            if module_grad_params:  # ,    \n",
    "                grad_params.extend(module_grad_params)\n",
    "        return grad_params\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()\n",
    "\n",
    "    def __repr__(self):\n",
    "        string = \"Sequential_upd (\\n\"\n",
    "        for i, module in enumerate(self.modules):\n",
    "            module_str = str(module)\n",
    "            indented_module_str = \"  \" + module_str.replace(\"\\n\", \"\\n  \")\n",
    "            if indented_module_str.endswith(\"\\n  \"):\n",
    "                indented_module_str = indented_module_str[:-2]\n",
    "            string += f\"  ({i}): {indented_module_str}\\n\"\n",
    "        string += \")\"\n",
    "        return string\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.     ,      ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, model_params, lr=1e-3, wd=0):\n",
    "        self.model_params = model_params\n",
    "        self.lr = lr\n",
    "        self.wd = wd  # L2 \n",
    "\n",
    "    def step(self, gradients):\n",
    "        for param, grad in zip(self.model_params, gradients):\n",
    "            if param is None or grad is None:\n",
    "                continue\n",
    "            if (self.wd > 0 and param.ndim > 1):\n",
    "                grad = grad + self.wd * param\n",
    "            update = self.lr * grad\n",
    "            if param.dtype != grad.dtype:\n",
    "                update = update.astype(param.dtype)\n",
    "            param -= update\n",
    "\n",
    "    def zero_grad(self, model):\n",
    "        model.zeroGradParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLR:\n",
    "    def __init__(self, optim, step_size, gamma=0.1):\n",
    "        self.optim = optim\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.epoch = 0\n",
    "    def step(self):\n",
    "        self.epoch += 1\n",
    "        if self.epoch % self.step_size == 0:\n",
    "            self.optim.lr = self.optim.lr * self.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred): \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def r2(y_pred, y_true):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def batcing(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler=None, X_train=None, y_train=None,\n",
    "          X_val=None, y_val=None, epochs=10, batch_size=32, metric_func=None, mode=\"classification\"):\n",
    "\n",
    "    train_loss_history, val_loss_history, train_metric_history, val_metric_history = [], [], [], []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_metric = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        for X_batch, y_batch in batcing(X_train, y_train, batch_size):\n",
    "            optimizer.zero_grad(model)\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "            grad = criterion.backward(y_pred, y_batch)\n",
    "            model.backward(X_batch, grad)\n",
    "            optimizer.step(model.getGradParameters())\n",
    "\n",
    "            train_loss += loss * X_batch.shape[0]\n",
    "            \n",
    "            if metric_func:\n",
    "                y_pred_proc = np.argmax(y_pred, axis=-1) if mode == \"classification\" else y_pred\n",
    "                metric = metric_func(y_batch, y_pred_proc)\n",
    "                train_metric += metric * X_batch.shape[0]\n",
    "            \n",
    "            train_samples += X_batch.shape[0]\n",
    "\n",
    "        model.evaluate()\n",
    "        val_loss = 0.0\n",
    "        val_metric = 0.0\n",
    "        val_samples = 0\n",
    "\n",
    "        for X_batch, y_batch in batcing(X_val, y_val, batch_size):\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "            val_loss += loss * X_batch.shape[0]\n",
    "            \n",
    "            if metric_func:\n",
    "                y_pred_proc = np.argmax(y_pred, axis=-1) if mode == \"classification\" else y_pred\n",
    "                metric = metric_func(y_batch, y_pred_proc)\n",
    "                val_metric += metric * X_batch.shape[0]\n",
    "            \n",
    "            val_samples += X_batch.shape[0]\n",
    "\n",
    "        avg_train_loss = train_loss / train_samples\n",
    "        avg_val_loss = val_loss / val_samples\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        if metric_func:\n",
    "            avg_train_metric = train_metric / train_samples\n",
    "            avg_val_metric = val_metric / val_samples\n",
    "            train_metric_history.append(avg_train_metric)\n",
    "            val_metric_history.append(avg_val_metric)\n",
    "\n",
    "        lr = optimizer.lr\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            new_lr = optimizer.lr\n",
    "            lr_changed = new_lr != lr\n",
    "        else:\n",
    "            lr_changed = False\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}\", end=\"\")\n",
    "        \n",
    "        if metric_func:\n",
    "            print(f\"| Train {str(metric)}: {avg_train_metric:.4f}| \"\n",
    "                  f\"Val {str(metric)}: {avg_val_metric:.4f}\", end=\"\")\n",
    "            print()\n",
    "\n",
    "\n",
    "    return (train_loss_history, val_loss_history, train_metric_history, val_metric_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  \n",
    "       .  FCNN, dropout, batchnorm, MSE.    .     ,    ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_true = np.random.randn(3, 5).astype(np.float32) * 2\n",
    "b_true = np.random.randn(3).astype(np.float32) * 5\n",
    "X = np.random.rand(10000, 5).astype(np.float32) * 10\n",
    "\n",
    "y = (X @ W_true.T + b_true + np.random.randn(10000, 3).astype(np.float32) * 0.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_model(n_features, n_targets, hidden_size, n_layers, activation=None, dropout_mode=0.0):\n",
    "    model = Sequential_upd()\n",
    "    last_size = n_features\n",
    "    for i in range(n_layers):\n",
    "        model.add(Linear(last_size, hidden_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ChannelwiseScaling(hidden_size))\n",
    "        model.add(activation())\n",
    "        if dropout_mode > 0:\n",
    "            model.add(Dropout(dropout_mode))\n",
    "        last_size = hidden_size\n",
    "    model.add(Linear(last_size, n_targets))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4   (,   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = create_regression_model(n_features=5, n_targets=3, hidden_size=32, n_layers=1, activation=ReLU, dropout_mode=0.1)\n",
    "model_medium = create_regression_model(n_features=5, n_targets=3, hidden_size=64, n_layers=2, activation=ELU, dropout_mode=0.2)\n",
    "model_large = create_regression_model(n_features=5, n_targets=3, hidden_size=128, n_layers=3, activation=Gelu, dropout_mode=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5   ( ,  - )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 494.6732 | Val Loss: 110.4759| Train 0.46577150407143436: -40859.9137| Val 0.46577150407143436: 0.3488\n",
      "Epoch 2/50: Train Loss: 86.0776 | Val Loss: 22.5822| Train 0.9160977003841749: 0.7254| Val 0.9160977003841749: 0.9139\n",
      "Epoch 3/50: Train Loss: 71.7146 | Val Loss: 22.8094| Train 0.892968766596205: 0.8007| Val 0.892968766596205: 0.9066\n",
      "Epoch 4/50: Train Loss: 66.0826 | Val Loss: 23.1388| Train 0.9032374335988371: 0.8165| Val 0.9032374335988371: 0.9054\n",
      "Epoch 5/50: Train Loss: 62.0966 | Val Loss: 23.4065| Train 0.9410051750454104: 0.8309| Val 0.9410051750454104: 0.9412\n",
      "Epoch 6/50: Train Loss: 61.1537 | Val Loss: 55.9502| Train 0.8399379233728163: 0.8326| Val 0.8399379233728163: 0.8867\n",
      "Epoch 7/50: Train Loss: 55.6463 | Val Loss: 13.5647| Train 0.9695718098701717: 0.8487| Val 0.9695718098701717: 0.9654\n",
      "Epoch 8/50: Train Loss: 53.5592 | Val Loss: 49.7808| Train 0.8857031157389454: 0.8551| Val 0.8857031157389454: 0.8798\n",
      "Epoch 9/50: Train Loss: 51.7355 | Val Loss: 9.1821| Train 0.9727777541925097: 0.8643| Val 0.9727777541925097: 0.9715\n",
      "Epoch 10/50: Train Loss: 54.3152 | Val Loss: 59.3765| Train 0.8955161359622673: 0.8575| Val 0.8955161359622673: 0.8818\n",
      "Epoch 11/50: Train Loss: 51.2554 | Val Loss: 11.1312| Train 0.9696837717087526: 0.8658| Val 0.9696837717087526: 0.9726\n",
      "Epoch 12/50: Train Loss: 49.6855 | Val Loss: 12.3904| Train 0.9525772474765272: 0.8688| Val 0.9525772474765272: 0.9514\n",
      "Epoch 13/50: Train Loss: 48.3011 | Val Loss: 28.9210| Train 0.8759041578077081: 0.8725| Val 0.8759041578077081: 0.8958\n",
      "Epoch 14/50: Train Loss: 47.9780 | Val Loss: 18.0720| Train 0.9308552433626605: 0.8728| Val 0.9308552433626605: 0.9309\n",
      "Epoch 15/50: Train Loss: 51.7687 | Val Loss: 14.9095| Train 0.9641608385530812: 0.8651| Val 0.9641608385530812: 0.9605\n",
      "Epoch 16/50: Train Loss: 48.6445 | Val Loss: 10.2452| Train 0.9717137617657482: 0.8712| Val 0.9717137617657482: 0.9678\n",
      "Epoch 17/50: Train Loss: 46.6601 | Val Loss: 31.2651| Train 0.9464246972932729: 0.8756| Val 0.9464246972932729: 0.9375\n",
      "Epoch 18/50: Train Loss: 48.8805 | Val Loss: 47.0142| Train 0.9177649167863452: 0.8730| Val 0.9177649167863452: 0.9333\n",
      "Epoch 19/50: Train Loss: 47.9161 | Val Loss: 39.5400| Train 0.944344069191502: 0.8747| Val 0.944344069191502: 0.9302\n",
      "Epoch 20/50: Train Loss: 47.8470 | Val Loss: 19.4527| Train 0.9559587035964036: 0.8753| Val 0.9559587035964036: 0.9534\n",
      "Epoch 21/50: Train Loss: 48.6568 | Val Loss: 9.5115| Train 0.9789196406363915: 0.8731| Val 0.9789196406363915: 0.9721\n",
      "Epoch 22/50: Train Loss: 46.3543 | Val Loss: 11.0309| Train 0.9451450967688243: 0.8778| Val 0.9451450967688243: 0.9504\n",
      "Epoch 23/50: Train Loss: 46.3468 | Val Loss: 30.0177| Train 0.9360357894492731: 0.8795| Val 0.9360357894492731: 0.9229\n",
      "Epoch 24/50: Train Loss: 45.7119 | Val Loss: 12.4379| Train 0.9793149702885872: 0.8793| Val 0.9793149702885872: 0.9657\n",
      "Epoch 25/50: Train Loss: 46.4867 | Val Loss: 45.6821| Train 0.8208943434986707: 0.8769| Val 0.8208943434986707: 0.8483\n",
      "Epoch 26/50: Train Loss: 48.4327 | Val Loss: 36.5931| Train 0.918234372679092: 0.8738| Val 0.918234372679092: 0.9058\n",
      "Epoch 27/50: Train Loss: 46.2236 | Val Loss: 121.4024| Train 0.47045525496732427: 0.8793| Val 0.47045525496732427: 0.5951\n",
      "Epoch 28/50: Train Loss: 43.4563 | Val Loss: 6.8554| Train 0.9774196500950044: 0.8856| Val 0.9774196500950044: 0.9771\n",
      "Epoch 29/50: Train Loss: 44.2103 | Val Loss: 33.7807| Train 0.9097388570098773: 0.8833| Val 0.9097388570098773: 0.8867\n",
      "Epoch 30/50: Train Loss: 45.4656 | Val Loss: 20.2031| Train 0.9189088372994503: 0.8798| Val 0.9189088372994503: 0.9363\n",
      "Epoch 31/50: Train Loss: 45.7131 | Val Loss: 10.1612| Train 0.9664498794526647: 0.8805| Val 0.9664498794526647: 0.9706\n",
      "Epoch 32/50: Train Loss: 43.9722 | Val Loss: 7.4661| Train 0.9852652025225194: 0.8862| Val 0.9852652025225194: 0.9780\n",
      "Epoch 33/50: Train Loss: 44.0326 | Val Loss: 16.0230| Train 0.9424900082506252: 0.8834| Val 0.9424900082506252: 0.9452\n",
      "Epoch 34/50: Train Loss: 44.1149 | Val Loss: 68.2434| Train 0.8118616539942322: 0.8873| Val 0.8118616539942322: 0.8189\n",
      "Epoch 35/50: Train Loss: 46.9851 | Val Loss: 17.2953| Train 0.9425510132613161: 0.8753| Val 0.9425510132613161: 0.9389\n",
      "Epoch 36/50: Train Loss: 46.7391 | Val Loss: 63.1622| Train 0.9053389269363956: 0.8800| Val 0.9053389269363956: 0.9157\n",
      "Epoch 37/50: Train Loss: 44.4174 | Val Loss: 102.8969| Train 0.8837557017687646: 0.8839| Val 0.8837557017687646: 0.8241\n",
      "Epoch 38/50: Train Loss: 42.9414 | Val Loss: 17.0524| Train 0.9447013207786162: 0.8880| Val 0.9447013207786162: 0.9507\n",
      "Epoch 39/50: Train Loss: 45.8253 | Val Loss: 61.1744| Train 0.9010690743352843: 0.8823| Val 0.9010690743352843: 0.8991\n",
      "Epoch 40/50: Train Loss: 45.1688 | Val Loss: 11.0838| Train 0.9804942280147424: 0.8825| Val 0.9804942280147424: 0.9714\n",
      "Epoch 41/50: Train Loss: 43.7414 | Val Loss: 19.1791| Train 0.9638152319158682: 0.8877| Val 0.9638152319158682: 0.9601\n",
      "Epoch 42/50: Train Loss: 45.5071 | Val Loss: 40.6421| Train 0.9008559873244079: 0.8820| Val 0.9008559873244079: 0.8990\n",
      "Epoch 43/50: Train Loss: 42.8234 | Val Loss: 60.5289| Train 0.8107541742348175: 0.8892| Val 0.8107541742348175: 0.8562\n",
      "Epoch 44/50: Train Loss: 44.9906 | Val Loss: 193.3952| Train 0.5381150929538666: 0.8836| Val 0.5381150929538666: 0.6915\n",
      "Epoch 45/50: Train Loss: 46.5759 | Val Loss: 72.6944| Train 0.9213438883203485: 0.8818| Val 0.9213438883203485: 0.9125\n",
      "Epoch 46/50: Train Loss: 45.9846 | Val Loss: 32.6253| Train 0.8782657617371844: 0.8809| Val 0.8782657617371844: 0.9025\n",
      "Epoch 47/50: Train Loss: 45.8547 | Val Loss: 42.4281| Train 0.9046962274512808: 0.8811| Val 0.9046962274512808: 0.9003\n",
      "Epoch 48/50: Train Loss: 46.5196 | Val Loss: 17.6304| Train 0.9408529078611535: 0.8790| Val 0.9408529078611535: 0.9486\n",
      "Epoch 49/50: Train Loss: 43.6770 | Val Loss: 7.3837| Train 0.98453383657608: 0.8862| Val 0.98453383657608: 0.9803\n",
      "Epoch 50/50: Train Loss: 45.0211 | Val Loss: 141.7215| Train 0.8218593929118909: 0.8809| Val 0.8218593929118909: 0.7922\n"
     ]
    }
   ],
   "source": [
    "criterion = MSECriterion()\n",
    "optimizer = SGD(model_small.getParameters(), 1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_loss, val_loss, train_metric, val_metric = train(model_small, criterion, optimizer, scheduler, X_train, y_train,\n",
    "                                                       X_val, y_val, epochs=50, batch_size=32, metric_func=r2, mode=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(train_metric) + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_metric, label=\"Training\")\n",
    "plt.title(\"Training R2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, label=\"Training\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  \n",
    "     MNIST.  , , , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1   MINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_val = MNIST(root=\"./data\", train=True, download=True, transform=ToTensor())\n",
    "test = MNIST(root=\"./data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_val.data.numpy().astype(np.float32) / 255.0\n",
    "y = train_val.targets.numpy()\n",
    "X_test = test.data.numpy().astype(np.float32) / 255.0\n",
    "y_test = test.targets.numpy()\n",
    "X = X[:, None, :, :]\n",
    "X_test = X_test[:, None, :, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model():\n",
    "    model = Sequential_upd()\n",
    "    model.add(Conv2d(in_channels=1, out_channels=8, kernel_size=5, padding=2))\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
    "    model.add(Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1))\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Linear(n_in=16 * 7 * 7, n_out=128))\n",
    "    model.add(ReLU())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Linear(n_in=128, n_out=10))\n",
    "    model.add(SoftMax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classification_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = MSECriterion()\n",
    "# optimizer = SGD(model.getParameters(), lr=1e-3)\n",
    "# train_loss, val_loss, train_metric, val_metric = train(model, criterion, optimizer, X_train, y_train, X_val, y_val,\n",
    "#                                                        epochs=1, batch_size=128, metric_func=accuracy, mode=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i'm done"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
